# Multi-Dataset Anomaly Detection - Enhanced Implementation
# Supporting both CSE-CIC-IDS2018 and MAWIFlow (2025) datasets

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

print("ðŸš€ Multi-Dataset Anomaly Detection System")
print("=" * 60)
print(f"Python version: {sys.version}")

# Import deep learning libraries
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    print(f"âœ“ PyTorch: {torch.__version__}")
    
    # Check for GPU availability
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ“ Device: {device}")
    if torch.cuda.is_available():
        print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
    
except ImportError as e:
    print(f"âœ— PyTorch not available: {e}")
    device = 'cpu'

try:
    from sklearn.model_selection import train_test_split, StratifiedKFold
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
    from sklearn.ensemble import IsolationForest
    print("âœ“ Scikit-learn imported")
except ImportError as e:
    print(f"âœ— Scikit-learn not available: {e}")

try:
    from imblearn.over_sampling import SMOTE
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.pipeline import Pipeline as ImbPipeline
    print("âœ“ Imbalanced-learn imported")
except ImportError as e:
    print(f"âœ— Imbalanced-learn not available: {e}")

class MultiDatasetLoader:
    """Enhanced data loader supporting multiple network traffic datasets"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.datasets = {}
        self.feature_mappings = {}
        
    def load_cic_ids2018(self, data_path="./CSE-CIC-IDS2018/", sample_size=None):
        """Load CSE-CIC-IDS2018 dataset"""
        print("\nðŸ“Š Loading CSE-CIC-IDS2018 Dataset...")
        
        # Common file patterns for CIC-IDS2018
        file_patterns = [
            "Wednesday-workingHours.pcap_ISCX.csv",
            "Tuesday-WorkingHours.pcap_ISCX.csv", 
            "Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv",
            "Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv",
            "Friday-WorkingHours-Morning.pcap_ISCX.csv",
            "Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv",
            "Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv"
        ]
        
        if not os.path.exists(data_path):
            print(f"âš ï¸  Path {data_path} not found. Creating synthetic CIC-IDS2018 data...")
            return self._create_synthetic_cic_data(sample_size or 15000)
        
        all_data = []
        for pattern in file_patterns:
            file_path = os.path.join(data_path, pattern)
            if os.path.exists(file_path):
                try:
                    print(f"   Loading {pattern}...")
                    df = pd.read_csv(file_path)
                    if sample_size and len(all_data) == 0:
                        df = df.sample(n=min(sample_size, len(df)), random_state=self.random_state)
                    all_data.append(df)
                except Exception as e:
                    print(f"   âœ— Error loading {pattern}: {e}")
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            print(f"âœ“ Loaded {len(combined_df):,} samples from CIC-IDS2018")
            combined_df['dataset_source'] = 'CIC-IDS2018'
            return combined_df
        else:
            print("âš ï¸  No CIC-IDS2018 files found. Creating synthetic data...")
            return self._create_synthetic_cic_data(sample_size or 15000)
    
    def load_mawiflow_2025(self, data_path="./MAWIFlow-2025/", sample_size=None):
        """Load MAWIFlow (2025) dataset"""
        print("\nðŸ“Š Loading MAWIFlow (2025) Dataset...")
        
        # Expected file patterns for MAWIFlow 2025
        file_patterns = [
            "mawiflow_2025_week1.csv",
            "mawiflow_2025_week2.csv", 
            "mawiflow_2025_ddos.csv",
            "mawiflow_2025_botnet.csv",
            "mawiflow_2025_normal.csv",
            "flows_labeled.csv",
            "backbone_traffic_2025.csv"
        ]
        
        if not os.path.exists(data_path):
            print(f"âš ï¸  Path {data_path} not found. Creating synthetic MAWIFlow data...")
            return self._create_synthetic_mawi_data(sample_size or 12000)
        
        all_data = []
        for pattern in file_patterns:
            file_path = os.path.join(data_path, pattern)
            if os.path.exists(file_path):
                try:
                    print(f"   Loading {pattern}...")
                    df = pd.read_csv(file_path)
                    if sample_size and len(all_data) == 0:
                        df = df.sample(n=min(sample_size, len(df)), random_state=self.random_state)
                    all_data.append(df)
                except Exception as e:
                    print(f"   âœ— Error loading {pattern}: {e}")
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            print(f"âœ“ Loaded {len(combined_df):,} samples from MAWIFlow (2025)")
            combined_df['dataset_source'] = 'MAWIFlow-2025'
            return combined_df
        else:
            print("âš ï¸  No MAWIFlow files found. Creating synthetic data...")
            return self._create_synthetic_mawi_data(sample_size or 12000)
    
    def _create_synthetic_cic_data(self, n_samples=15000):
        """Create synthetic data mimicking CIC-IDS2018 characteristics"""
        print(f"ðŸ”„ Generating {n_samples:,} synthetic CIC-IDS2018 samples...")
        
        np.random.seed(self.random_state)
        
        # CIC-IDS2018 feature set (subset of most important features)
        features = [
            'Flow_Duration', 'Total_Fwd_Packets', 'Total_Backward_Packets',
            'Total_Length_of_Fwd_Packets', 'Total_Length_of_Bwd_Packets',
            'Fwd_Packet_Length_Max', 'Fwd_Packet_Length_Min', 'Fwd_Packet_Length_Mean',
            'Bwd_Packet_Length_Max', 'Bwd_Packet_Length_Min', 'Bwd_Packet_Length_Mean',
            'Flow_Bytes_s', 'Flow_Packets_s', 'Flow_IAT_Mean', 'Flow_IAT_Std',
            'Fwd_IAT_Total', 'Fwd_IAT_Mean', 'Fwd_IAT_Std', 'Bwd_IAT_Total', 'Bwd_IAT_Mean',
            'Fwd_PSH_Flags', 'Bwd_PSH_Flags', 'Fwd_URG_Flags', 'Bwd_URG_Flags',
            'Fwd_Header_Length', 'Bwd_Header_Length', 'Fwd_Packets_s', 'Bwd_Packets_s',
            'Min_Packet_Length', 'Max_Packet_Length', 'Packet_Length_Mean', 'Packet_Length_Std',
            'FIN_Flag_Count', 'SYN_Flag_Count', 'RST_Flag_Count', 'PSH_Flag_Count',
            'ACK_Flag_Count', 'URG_Flag_Count', 'Average_Packet_Size', 'Avg_Fwd_Segment_Size'
        ]
        
        # Attack type distribution for CIC-IDS2018
        attack_types = ['BENIGN', 'DoS', 'DDoS', 'Botnet', 'Web Attack', 'Brute Force', 'Infiltration']
        attack_probs = [0.75, 0.08, 0.05, 0.04, 0.03, 0.03, 0.02]
        
        data = {}
        
        # Generate realistic network flow features
        for feature in features:
            if 'Duration' in feature:
                data[feature] = np.random.exponential(50000, n_samples)  # Flow duration in microseconds
            elif 'Packets' in feature and 's' in feature:
                data[feature] = np.random.gamma(2, 10, n_samples)  # Packets per second
            elif 'Bytes' in feature and 's' in feature:
                data[feature] = np.random.gamma(3, 1000, n_samples)  # Bytes per second
            elif 'Length' in feature:
                data[feature] = np.random.gamma(2, 200, n_samples)  # Packet lengths
            elif 'IAT' in feature:
                data[feature] = np.random.exponential(1000, n_samples)  # Inter-arrival times
            elif 'Flag' in feature:
                data[feature] = np.random.poisson(0.5, n_samples)  # Flag counts
            else:
                data[feature] = np.random.gamma(1.5, 50, n_samples)  # Generic features
        
        # Generate labels
        labels = np.random.choice(attack_types, n_samples, p=attack_probs)
        
        # Create DataFrame
        df = pd.DataFrame(data)
        df['Label'] = labels
        df['dataset_source'] = 'CIC-IDS2018'
        
        # Add some attack-specific patterns
        attack_mask = df['Label'] != 'BENIGN'
        df.loc[attack_mask, 'Flow_Bytes_s'] *= np.random.uniform(2, 5, attack_mask.sum())
        df.loc[attack_mask, 'Flow_Packets_s'] *= np.random.uniform(1.5, 3, attack_mask.sum())
        
        print(f"âœ“ Generated CIC-IDS2018 synthetic data: {df.shape}")
        return df
    
    def _create_synthetic_mawi_data(self, n_samples=12000):
        """Create synthetic data mimicking MAWIFlow (2025) characteristics"""
        print(f"ðŸ”„ Generating {n_samples:,} synthetic MAWIFlow (2025) samples...")
        
        np.random.seed(self.random_state + 1)  # Different seed for diversity
        
        # MAWIFlow 2025 feature set (modern network flow features)
        features = [
            'flow_duration_ms', 'src_port', 'dst_port', 'protocol',
            'total_fwd_packets', 'total_bwd_packets', 'total_bytes_fwd', 'total_bytes_bwd',
            'fwd_pkt_len_max', 'fwd_pkt_len_min', 'fwd_pkt_len_mean', 'fwd_pkt_len_std',
            'bwd_pkt_len_max', 'bwd_pkt_len_min', 'bwd_pkt_len_mean', 'bwd_pkt_len_std',
            'flow_bytes_per_sec', 'flow_pkts_per_sec', 'flow_iat_mean', 'flow_iat_std',
            'fwd_iat_total', 'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max', 'fwd_iat_min',
            'bwd_iat_total', 'bwd_iat_mean', 'bwd_iat_std', 'bwd_iat_max', 'bwd_iat_min',
            'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags',
            'fwd_header_len', 'bwd_header_len', 'min_pkt_len', 'max_pkt_len',
            'pkt_len_mean', 'pkt_len_std', 'pkt_len_var', 'fin_flag_cnt', 'syn_flag_cnt',
            'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'ece_flag_cnt'
        ]
        
        # Modern attack types in MAWIFlow 2025
        attack_types = ['Normal', 'DDoS', 'Botnet_2025', 'APT', 'Cryptomining', 'IoT_Attack', 'Zero_Day']
        attack_probs = [0.70, 0.10, 0.08, 0.05, 0.03, 0.02, 0.02]
        
        data = {}
        
        # Generate modern network flow features
        for feature in features:
            if 'duration' in feature:
                data[feature] = np.random.lognormal(8, 2, n_samples)  # Flow duration in ms
            elif 'port' in feature:
                data[feature] = np.random.choice([80, 443, 22, 21, 25, 53, 8080, 3389] + 
                                               list(np.random.randint(1024, 65535, 20)), n_samples)
            elif 'protocol' in feature:
                data[feature] = np.random.choice([6, 17, 1], n_samples, p=[0.7, 0.25, 0.05])  # TCP, UDP, ICMP
            elif 'bytes' in feature and 'sec' in feature:
                data[feature] = np.random.lognormal(10, 2, n_samples)  # Modern high-speed flows
            elif 'pkts' in feature and 'sec' in feature:
                data[feature] = np.random.gamma(3, 15, n_samples)  # Modern packet rates
            elif 'len' in feature:
                data[feature] = np.random.gamma(2, 250, n_samples)  # Modern packet sizes
            elif 'iat' in feature:
                data[feature] = np.random.exponential(500, n_samples)  # Inter-arrival times
            elif 'flag' in feature:
                data[feature] = np.random.poisson(0.8, n_samples)  # Flag counts
            else:
                data[feature] = np.random.gamma(2, 75, n_samples)  # Generic features
        
        # Generate labels
        labels = np.random.choice(attack_types, n_samples, p=attack_probs)
        
        # Create DataFrame
        df = pd.DataFrame(data)
        df['Label'] = labels
        df['dataset_source'] = 'MAWIFlow-2025'
        
        # Add modern attack patterns
        attack_mask = df['Label'] != 'Normal'
        df.loc[attack_mask, 'flow_bytes_per_sec'] *= np.random.uniform(3, 8, attack_mask.sum())
        
        # Add specific patterns for modern attacks
        crypto_mask = df['Label'] == 'Cryptomining'
        if crypto_mask.sum() > 0:
            df.loc[crypto_mask, 'flow_pkts_per_sec'] *= np.random.uniform(0.1, 0.5, crypto_mask.sum())
        
        iot_mask = df['Label'] == 'IoT_Attack'
        if iot_mask.sum() > 0:
            df.loc[iot_mask, 'pkt_len_mean'] *= np.random.uniform(0.3, 0.7, iot_mask.sum())
        
        print(f"âœ“ Generated MAWIFlow (2025) synthetic data: {df.shape}")
        return df
    
    def combine_datasets(self, cic_data, mawi_data, balance=True):
        """Combine both datasets with optional balancing"""
        print("\nðŸ”„ Combining CIC-IDS2018 and MAWIFlow (2025) datasets...")
        
        # Standardize column names
        cic_data = self._standardize_columns(cic_data, 'CIC-IDS2018')
        mawi_data = self._standardize_columns(mawi_data, 'MAWIFlow-2025')
        
        # Find common features
        cic_features = set(cic_data.columns) - {'Label', 'dataset_source'}
        mawi_features = set(mawi_data.columns) - {'Label', 'dataset_source'}
        common_features = list(cic_features.intersection(mawi_features))
        
        print(f"   Common features: {len(common_features)}")
        print(f"   CIC-only features: {len(cic_features - mawi_features)}")
        print(f"   MAWI-only features: {len(mawi_features - cic_features)}")
        
        # Keep only common features for fair comparison
        keep_cols = common_features + ['Label', 'dataset_source']
        cic_subset = cic_data[keep_cols].copy()
        mawi_subset = mawi_data[keep_cols].copy()
        
        # Combine datasets
        combined_df = pd.concat([cic_subset, mawi_subset], ignore_index=True)
        
        if balance:
            # Balance the dataset sizes
            min_size = min(len(cic_subset), len(mawi_subset))
            cic_balanced = cic_subset.sample(n=min_size, random_state=self.random_state)
            mawi_balanced = mawi_subset.sample(n=min_size, random_state=self.random_state)
            combined_df = pd.concat([cic_balanced, mawi_balanced], ignore_index=True)
            print(f"   Balanced combined dataset: {len(combined_df):,} samples")
        else:
            print(f"   Full combined dataset: {len(combined_df):,} samples")
        
        # Unified label mapping
        combined_df['Label_Binary'] = combined_df['Label'].apply(
            lambda x: 0 if x in ['BENIGN', 'Normal'] else 1
        )
        
        return combined_df
    
    def _standardize_columns(self, df, source):
        """Standardize column names between datasets"""
        df = df.copy()
        
        if source == 'CIC-IDS2018':
            # Map CIC column names to standard names
            column_mapping = {
                'Flow_Duration': 'flow_duration',
                'Total_Fwd_Packets': 'total_fwd_packets',
                'Total_Backward_Packets': 'total_bwd_packets',
                'Flow_Bytes_s': 'flow_bytes_per_sec',
                'Flow_Packets_s': 'flow_pkts_per_sec',
                'Fwd_Packet_Length_Mean': 'fwd_pkt_len_mean',
                'Bwd_Packet_Length_Mean': 'bwd_pkt_len_mean',
                'Flow_IAT_Mean': 'flow_iat_mean',
                'Flow_IAT_Std': 'flow_iat_std',
                'Packet_Length_Mean': 'pkt_len_mean',
                'Packet_Length_Std': 'pkt_len_std'
            }
        else:  # MAWIFlow-2025
            # Map MAWI column names to standard names
            column_mapping = {
                'flow_duration_ms': 'flow_duration',
                'flow_bytes_per_sec': 'flow_bytes_per_sec',
                'flow_pkts_per_sec': 'flow_pkts_per_sec',
                'total_fwd_packets': 'total_fwd_packets',
                'total_bwd_packets': 'total_bwd_packets',
                'fwd_pkt_len_mean': 'fwd_pkt_len_mean',
                'bwd_pkt_len_mean': 'bwd_pkt_len_mean',
                'flow_iat_mean': 'flow_iat_mean',
                'flow_iat_std': 'flow_iat_std',
                'pkt_len_mean': 'pkt_len_mean',
                'pkt_len_std': 'pkt_len_std'
            }
        
        # Apply mapping
        for old_name, new_name in column_mapping.items():
            if old_name in df.columns:
                df = df.rename(columns={old_name: new_name})
        
        return df

# Initialize the multi-dataset loader
print("\nðŸš€ Initializing Multi-Dataset Loader...")
loader = MultiDatasetLoader(random_state=RANDOM_STATE)

# Load both datasets
print("\n" + "="*60)
print("ðŸ“¥ LOADING DATASETS")
print("="*60)

# Load CIC-IDS2018 dataset
cic_data = loader.load_cic_ids2018(sample_size=15000)
print(f"\nCIC-IDS2018 Dataset Shape: {cic_data.shape}")
print(f"CIC-IDS2018 Label Distribution:")
if 'Label' in cic_data.columns:
    print(cic_data['Label'].value_counts())

# Load MAWIFlow 2025 dataset  
mawi_data = loader.load_mawiflow_2025(sample_size=12000)
print(f"\nMAWIFlow (2025) Dataset Shape: {mawi_data.shape}")
print(f"MAWIFlow (2025) Label Distribution:")
if 'Label' in mawi_data.columns:
    print(mawi_data['Label'].value_counts())

# Combine datasets
combined_data = loader.combine_datasets(cic_data, mawi_data, balance=True)
print(f"\nðŸŽ¯ Combined Dataset Overview:")
print(f"   Total samples: {len(combined_data):,}")
print(f"   Total features: {len([col for col in combined_data.columns if col not in ['Label', 'Label_Binary', 'dataset_source']])}")
print(f"   Dataset sources: {combined_data['dataset_source'].value_counts().to_dict()}")
print(f"   Binary label distribution: {combined_data['Label_Binary'].value_counts().to_dict()}")

# Display sample data
print(f"\nðŸ“Š Sample Combined Data:")
print(combined_data.head())

print("\nâœ… Multi-Dataset Loading Complete!")
print("="*60)
print("ðŸ“ˆ Ready for Enhanced Multi-Dataset Training!")
print("   - Cross-dataset validation available")
print("   - Improved generalization expected") 
print("   - Enhanced attack coverage achieved")
print("="*60)