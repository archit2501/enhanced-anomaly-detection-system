{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a928c7",
   "metadata": {},
   "source": [
    "# Anomaly Detection in Cloud Network Traffic using CSE-CIC-IDS2018 Dataset\n",
    "\n",
    "## Deep Learning Comparison: LSTM, CNN, and Autoencoder Models\n",
    "\n",
    "This notebook implements an end-to-end anomaly detection system for network traffic analysis using the CSE-CIC-IDS2018 dataset. We compare three deep learning approaches:\n",
    "\n",
    "1. **LSTM** - For sequential analysis of network flow features\n",
    "2. **1D CNN** - For local feature extraction from flow vectors  \n",
    "3. **Autoencoder** - For unsupervised anomaly detection\n",
    "\n",
    "### Project Objectives:\n",
    "- Handle severe class imbalance in network traffic data\n",
    "- Implement scalable deep learning models\n",
    "- Provide comprehensive comparative analysis\n",
    "- Achieve high accuracy (LSTM ~94%, CNN ~92%) with proper evaluation metrics\n",
    "\n",
    "### Dataset: CSE-CIC-IDS2018\n",
    "- Contains realistic network traffic with various attack types\n",
    "- Includes DoS, Botnet, Brute Force, Web Attacks, and Normal traffic\n",
    "- Highly imbalanced dataset requiring special handling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1031788",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "Install and import all required libraries for the anomaly detection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf653b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment for first run)\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install scikit-learn==1.3.0 pandas==2.0.3 numpy==1.24.3 matplotlib==3.7.2 seaborn==0.12.2\n",
    "# !pip install imbalanced-learn==0.11.0 tqdm==4.65.0 plotly==5.15.0\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Deep learning frameworks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Scikit-learn for preprocessing and metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                           precision_recall_fscore_support, roc_auc_score, \n",
    "                           precision_recall_curve, roc_curve, auc)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_STATE)\n",
    "    torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "\n",
    "# Configure device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11c29b",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Exploration\n",
    "\n",
    "Load the CSE-CIC-IDS2018 dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53786453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_PATH = \"./CSE-CIC-IDS2018/\"  # Update this path to your dataset location\n",
    "\n",
    "# CSE-CIC-IDS2018 files mapping\n",
    "DATASET_FILES = {\n",
    "    \"Normal\": [\"Thuesday-20-02-2018_TrafficForML_CIC_IoT.csv\"],\n",
    "    \"DoS\": [\"Thuesday-20-02-2018_TrafficForML_CIC_IoT.csv\", \n",
    "            \"Wednesday-21-02-2018_TrafficForML_CIC_IoT.csv\"],\n",
    "    \"DDoS\": [\"Thursday-22-02-2018_TrafficForML_CIC_IoT.csv\"],\n",
    "    \"Botnet\": [\"Friday-23-02-2018_TrafficForML_CIC_IoT.csv\"],\n",
    "    \"Web_Attack\": [\"Thursday-22-02-2018_TrafficForML_CIC_IoT.csv\"],\n",
    "    \"Brute_Force\": [\"Wednesday-28-02-2018_TrafficForML_CIC_IoT.csv\", \n",
    "                    \"Thursday-01-03-2018_TrafficForML_CIC_IoT.csv\"],\n",
    "    \"Infiltration\": [\"Friday-02-03-2018_TrafficForML_CIC_IoT.csv\"]\n",
    "}\n",
    "\n",
    "def load_cic_ids2018_data(data_path, sample_size=None, file_list=None):\n",
    "    \"\"\"\n",
    "    Load CSE-CIC-IDS2018 dataset from CSV files\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the dataset directory\n",
    "        sample_size: Number of samples to load (None for all)\n",
    "        file_list: Specific files to load (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Combined dataset\n",
    "    \"\"\"\n",
    "    print(\"Loading CSE-CIC-IDS2018 dataset...\")\n",
    "    \n",
    "    # For demo purposes, create synthetic data if files not available\n",
    "    if not os.path.exists(data_path):\n",
    "        print(\"Dataset path not found. Creating synthetic data for demonstration...\")\n",
    "        return create_synthetic_network_data(sample_size or 10000)\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Get list of CSV files\n",
    "    if file_list is None:\n",
    "        file_list = [f for f in os.listdir(data_path) if f.endswith('.csv')]\n",
    "    \n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Loading {file}...\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                if sample_size and len(all_data) == 0:\n",
    "                    df = df.sample(n=min(sample_size, len(df)), random_state=RANDOM_STATE)\n",
    "                all_data.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"Successfully loaded {len(combined_df)} samples from {len(all_data)} files\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No data files found. Creating synthetic data...\")\n",
    "        return create_synthetic_network_data(sample_size or 10000)\n",
    "\n",
    "def create_synthetic_network_data(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Create synthetic network traffic data for demonstration\n",
    "    \"\"\"\n",
    "    print(f\"Creating {n_samples} synthetic network traffic samples...\")\n",
    "    \n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    \n",
    "    # Define feature names (based on CIC-IDS2018 features)\n",
    "    features = [\n",
    "        'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',\n",
    "        'Total Length of Fwd Packets', 'Total Length of Bwd Packets',\n",
    "        'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
    "        'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean',\n",
    "        'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std',\n",
    "        'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Bwd IAT Total',\n",
    "        'Bwd IAT Mean', 'Bwd IAT Std', 'Fwd PSH Flags', 'Bwd PSH Flags',\n",
    "        'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\n",
    "        'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length',\n",
    "        'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance',\n",
    "        'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count',\n",
    "        'ACK Flag Count', 'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count',\n",
    "        'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\n",
    "        'Avg Bwd Segment Size', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk',\n",
    "        'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk',\n",
    "        'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
    "        'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init Win bytes forward',\n",
    "        'Init Win bytes backward', 'act data pkt fwd', 'min seg size forward',\n",
    "        'Active Mean', 'Active Std', 'Active Max', 'Active Min',\n",
    "        'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
    "    ]\n",
    "    \n",
    "    # Create synthetic data\n",
    "    data = {}\n",
    "    \n",
    "    # Generate features with different distributions\n",
    "    for i, feature in enumerate(features):\n",
    "        if 'Bytes' in feature or 'Length' in feature or 'Size' in feature:\n",
    "            # Packet size features - log-normal distribution\n",
    "            data[feature] = np.random.lognormal(mean=6, sigma=2, size=n_samples)\n",
    "        elif 'Time' in feature or 'IAT' in feature or 'Duration' in feature:\n",
    "            # Time-based features - exponential distribution\n",
    "            data[feature] = np.random.exponential(scale=1000, size=n_samples)\n",
    "        elif 'Flag' in feature or 'Count' in feature:\n",
    "            # Flag counts - Poisson distribution\n",
    "            data[feature] = np.random.poisson(lam=2, size=n_samples)\n",
    "        elif 'Ratio' in feature or '/s' in feature:\n",
    "            # Rate features - gamma distribution\n",
    "            data[feature] = np.random.gamma(shape=2, scale=0.5, size=n_samples)\n",
    "        else:\n",
    "            # Other features - normal distribution\n",
    "            data[feature] = np.random.normal(loc=0, scale=1, size=n_samples)\n",
    "    \n",
    "    # Create labels with class imbalance\n",
    "    attack_types = ['BENIGN', 'DoS Hulk', 'DoS GoldenEye', 'DoS slowloris', 'DoS Slowhttptest',\n",
    "                   'DDoS', 'Bot', 'FTP-Patator', 'SSH-Patator', 'Web Attack – Brute Force',\n",
    "                   'Web Attack – XSS', 'Web Attack – Sql Injection', 'Infiltration']\n",
    "    \n",
    "    # Create highly imbalanced distribution (realistic for network traffic)\n",
    "    label_probs = [0.8, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.005, 0.005]\n",
    "    \n",
    "    labels = np.random.choice(attack_types, size=n_samples, p=label_probs)\n",
    "    data['Label'] = labels\n",
    "    \n",
    "    # Add some categorical features\n",
    "    data['Protocol'] = np.random.choice(['TCP', 'UDP', 'ICMP'], size=n_samples, p=[0.7, 0.25, 0.05])\n",
    "    data['Source Port'] = np.random.randint(1024, 65535, size=n_samples)\n",
    "    data['Destination Port'] = np.random.choice([80, 443, 22, 21, 25, 53, 110, 143], \n",
    "                                               size=n_samples, p=[0.3, 0.3, 0.1, 0.05, 0.05, 0.1, 0.05, 0.05])\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add some correlations for attack types\n",
    "    attack_mask = df['Label'] != 'BENIGN'\n",
    "    df.loc[attack_mask, 'SYN Flag Count'] *= 2  # Attacks often have more SYN flags\n",
    "    df.loc[attack_mask, 'Flow Duration'] *= 0.5  # Attacks often have shorter flows\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"CSE-CIC-IDS2018 DATASET LOADING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data (using synthetic data for demonstration)\n",
    "df = load_cic_ids2018_data(DATA_PATH, sample_size=50000)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Features: {df.shape[1] - 1}\")  # Excluding label column\n",
    "print(f\"Samples: {df.shape[0]}\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore class distribution\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Label distribution\n",
    "label_counts = df['Label'].value_counts()\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "total_samples = len(df)\n",
    "benign_samples = label_counts.get('BENIGN', 0)\n",
    "attack_samples = total_samples - benign_samples\n",
    "\n",
    "print(f\"\\nClass Imbalance Analysis:\")\n",
    "print(f\"Total samples: {total_samples:,}\")\n",
    "print(f\"Benign samples: {benign_samples:,} ({benign_samples/total_samples*100:.1f}%)\")\n",
    "print(f\"Attack samples: {attack_samples:,} ({attack_samples/total_samples*100:.1f}%)\")\n",
    "print(f\"Imbalance ratio (Benign:Attack): {benign_samples/attack_samples:.1f}:1\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot\n",
    "label_counts.plot(kind='bar', ax=axes[0], color='skyblue', alpha=0.8)\n",
    "axes[0].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Attack Type')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Pie chart for top classes\n",
    "top_classes = label_counts.head(8)\n",
    "other_count = label_counts.tail(-8).sum()\n",
    "if other_count > 0:\n",
    "    top_classes['Others'] = other_count\n",
    "\n",
    "top_classes.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Class Distribution (Top Classes)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percent\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(f\"\\nColumns with missing values: {(missing_values > 0).sum()}\")\n",
    "print(f\"Total missing values: {missing_values.sum()}\")\n",
    "\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\nTop 10 columns with missing values:\")\n",
    "    print(missing_df.head(10))\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "# Check for infinite values\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INFINITE VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "infinite_counts = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    inf_count = np.isinf(df[col]).sum()\n",
    "    if inf_count > 0:\n",
    "        infinite_counts[col] = inf_count\n",
    "\n",
    "if infinite_counts:\n",
    "    print(f\"Columns with infinite values: {len(infinite_counts)}\")\n",
    "    for col, count in sorted(infinite_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"{col}: {count} infinite values\")\n",
    "else:\n",
    "    print(\"No infinite values found!\")\n",
    "\n",
    "# Data types analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA TYPES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nData types summary:\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "print(dtype_counts)\n",
    "\n",
    "print(f\"\\nNumerical features: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"Categorical features: {len(df.select_dtypes(include=['object']).columns)}\")\n",
    "\n",
    "# Display categorical features\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\nCategorical columns: {categorical_cols}\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    unique_vals = df[col].nunique()\n",
    "    print(f\"{col}: {unique_vals} unique values\")\n",
    "    if unique_vals <= 10:\n",
    "        print(f\"  Values: {df[col].unique()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dcec13",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Clean the dataset and prepare features for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e9ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cic_ids2018(df):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing for CSE-CIC-IDS2018 dataset\n",
    "    \n",
    "    Args:\n",
    "        df: Raw dataset DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        X: Processed features\n",
    "        y: Processed labels\n",
    "        feature_names: List of feature names\n",
    "        label_encoder: Fitted label encoder\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA PREPROCESSING PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Make a copy to avoid modifying original data\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    print(\"Step 1: Handling missing values...\")\n",
    "    initial_missing = df_processed.isnull().sum().sum()\n",
    "    print(f\"Initial missing values: {initial_missing}\")\n",
    "    \n",
    "    # Fill missing values with median for numerical columns\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'Label':  # Don't fill missing labels\n",
    "            df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
    "    \n",
    "    # Fill missing values with mode for categorical columns\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if col != 'Label':  # Don't fill missing labels\n",
    "            df_processed[col] = df_processed[col].fillna(df_processed[col].mode()[0])\n",
    "    \n",
    "    final_missing = df_processed.isnull().sum().sum()\n",
    "    print(f\"Final missing values: {final_missing}\")\n",
    "    \n",
    "    # 2. Handle infinite values\n",
    "    print(\"\\nStep 2: Handling infinite values...\")\n",
    "    infinite_count = 0\n",
    "    for col in numeric_cols:\n",
    "        if col != 'Label':\n",
    "            inf_mask = np.isinf(df_processed[col])\n",
    "            infinite_count += inf_mask.sum()\n",
    "            if inf_mask.any():\n",
    "                # Replace infinite values with column maximum\n",
    "                max_val = df_processed[col][~inf_mask].max()\n",
    "                df_processed.loc[inf_mask, col] = max_val\n",
    "    \n",
    "    print(f\"Infinite values handled: {infinite_count}\")\n",
    "    \n",
    "    # 3. Remove duplicate rows\n",
    "    print(\"\\nStep 3: Removing duplicates...\")\n",
    "    initial_rows = len(df_processed)\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    final_rows = len(df_processed)\n",
    "    print(f\"Duplicates removed: {initial_rows - final_rows}\")\n",
    "    \n",
    "    # 4. Handle categorical features\n",
    "    print(\"\\nStep 4: Encoding categorical features...\")\n",
    "    \n",
    "    # Separate features and labels\n",
    "    if 'Label' in df_processed.columns:\n",
    "        X = df_processed.drop('Label', axis=1)\n",
    "        y = df_processed['Label']\n",
    "    else:\n",
    "        X = df_processed\n",
    "        y = None\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "    print(f\"Categorical features to encode: {list(categorical_features)}\")\n",
    "    \n",
    "    # Simple label encoding for categorical features\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # 5. Encode target labels\n",
    "    if y is not None:\n",
    "        print(\"\\nStep 5: Encoding target labels...\")\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y)\n",
    "        \n",
    "        print(f\"Classes: {len(label_encoder.classes_)}\")\n",
    "        print(\"Label mapping:\")\n",
    "        for i, label in enumerate(label_encoder.classes_):\n",
    "            print(f\"  {i}: {label}\")\n",
    "    else:\n",
    "        y_encoded = None\n",
    "        label_encoder = None\n",
    "    \n",
    "    # 6. Feature selection and cleaning\n",
    "    print(\"\\nStep 6: Feature selection...\")\n",
    "    \n",
    "    # Remove features with very low variance\n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns\n",
    "    low_variance_features = []\n",
    "    \n",
    "    for col in numeric_features:\n",
    "        if X[col].var() < 1e-8:  # Very low variance threshold\n",
    "            low_variance_features.append(col)\n",
    "    \n",
    "    if low_variance_features:\n",
    "        print(f\"Removing {len(low_variance_features)} low-variance features\")\n",
    "        X = X.drop(columns=low_variance_features)\n",
    "    \n",
    "    # Remove highly correlated features (correlation > 0.95)\n",
    "    print(\"\\nRemoving highly correlated features...\")\n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_features) > 1:\n",
    "        corr_matrix = X[numeric_features].corr().abs()\n",
    "        upper_triangle = corr_matrix.where(\n",
    "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "        )\n",
    "        \n",
    "        high_corr_features = [column for column in upper_triangle.columns \n",
    "                            if any(upper_triangle[column] > 0.95)]\n",
    "        \n",
    "        if high_corr_features:\n",
    "            print(f\"Removing {len(high_corr_features)} highly correlated features\")\n",
    "            X = X.drop(columns=high_corr_features)\n",
    "    \n",
    "    # 7. Final feature preparation\n",
    "    feature_names = X.columns.tolist()\n",
    "    X_array = X.values.astype(np.float32)\n",
    "    \n",
    "    print(f\"\\nFinal dataset shape: {X_array.shape}\")\n",
    "    print(f\"Features selected: {len(feature_names)}\")\n",
    "    \n",
    "    if y_encoded is not None:\n",
    "        print(f\"Classes: {len(np.unique(y_encoded))}\")\n",
    "        class_counts = pd.Series(y_encoded).value_counts().sort_index()\n",
    "        print(\"Class distribution:\")\n",
    "        for i, count in enumerate(class_counts):\n",
    "            class_name = label_encoder.classes_[i]\n",
    "            print(f\"  {i} ({class_name}): {count} samples ({count/len(y_encoded)*100:.1f}%)\")\n",
    "    \n",
    "    return X_array, y_encoded, feature_names, label_encoder, label_encoders\n",
    "\n",
    "# Apply preprocessing\n",
    "X, y, feature_names, label_encoder, categorical_encoders = preprocess_cic_ids2018(df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Label vector shape: {y.shape}\")\n",
    "print(f\"Data type: {X.dtype}\")\n",
    "print(f\"Memory usage: {X.nbytes / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b17b45",
   "metadata": {},
   "source": [
    "## 4. Class Imbalance Handling\n",
    "\n",
    "Address the severe class imbalance using SMOTE, undersampling, and weighted loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_class_imbalance(X, y, strategy='combined', sampling_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Handle class imbalance using various techniques\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Label vector\n",
    "        strategy: 'smote', 'undersample', 'combined', or 'none'\n",
    "        sampling_ratio: Ratio for resampling\n",
    "        \n",
    "    Returns:\n",
    "        X_resampled, y_resampled: Resampled data\n",
    "        class_weights: Computed class weights for loss functions\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CLASS IMBALANCE HANDLING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate class weights for weighted loss\n",
    "    unique_classes = np.unique(y)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=unique_classes,\n",
    "        y=y\n",
    "    )\n",
    "    \n",
    "    # Convert to dictionary format for PyTorch\n",
    "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "    \n",
    "    print(\"Original class distribution:\")\n",
    "    original_counts = pd.Series(y).value_counts().sort_index()\n",
    "    for i, count in enumerate(original_counts):\n",
    "        class_name = label_encoder.classes_[i]\n",
    "        weight = class_weights[i]\n",
    "        print(f\"  Class {i} ({class_name}): {count} samples (weight: {weight:.3f})\")\n",
    "    \n",
    "    print(f\"\\nClass weight tensor: {torch.tensor(class_weights, dtype=torch.float32)}\")\n",
    "    \n",
    "    if strategy == 'none':\n",
    "        print(\"\\nNo resampling applied.\")\n",
    "        return X, y, torch.tensor(class_weights, dtype=torch.float32)\n",
    "    \n",
    "    # Apply resampling strategies\n",
    "    print(f\"\\nApplying resampling strategy: {strategy}\")\n",
    "    \n",
    "    if strategy == 'smote':\n",
    "        # Use SMOTE for oversampling minority classes\n",
    "        # Limit k_neighbors based on smallest class size\n",
    "        min_class_size = min(pd.Series(y).value_counts())\n",
    "        k_neighbors = min(5, min_class_size - 1) if min_class_size > 1 else 1\n",
    "        \n",
    "        smote = SMOTE(\n",
    "            sampling_strategy='auto',\n",
    "            k_neighbors=k_neighbors,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        \n",
    "    elif strategy == 'undersample':\n",
    "        # Use random undersampling for majority class\n",
    "        undersampler = RandomUnderSampler(\n",
    "            sampling_strategy='auto',\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "        \n",
    "    elif strategy == 'combined':\n",
    "        # Combine SMOTE for minority classes and undersampling for majority\n",
    "        \n",
    "        # First, apply SMOTE to increase minority classes\n",
    "        min_class_size = min(pd.Series(y).value_counts())\n",
    "        k_neighbors = min(3, min_class_size - 1) if min_class_size > 1 else 1\n",
    "        \n",
    "        # Calculate target sampling strategy for SMOTE\n",
    "        class_counts = pd.Series(y).value_counts()\n",
    "        majority_count = class_counts.max()\n",
    "        target_count = int(majority_count * sampling_ratio)\n",
    "        \n",
    "        # SMOTE strategy: bring minority classes to target_count\n",
    "        smote_strategy = {}\\n        for class_idx in class_counts.index:\\n            if class_counts[class_idx] < target_count:\\n                smote_strategy[class_idx] = target_count\\n        \\n        if smote_strategy:\\n            smote = SMOTE(\\n                sampling_strategy=smote_strategy,\\n                k_neighbors=k_neighbors,\\n                random_state=RANDOM_STATE\\n            )\\n            X_temp, y_temp = smote.fit_resample(X, y)\\n        else:\\n            X_temp, y_temp = X, y\\n        \\n        # Then apply undersampling to reduce majority class\\n        temp_counts = pd.Series(y_temp).value_counts()\\n        max_count = int(temp_counts.max() * 0.7)  # Reduce majority class\\n        \\n        undersample_strategy = {}\\n        for class_idx in temp_counts.index:\\n            if temp_counts[class_idx] > max_count:\\n                undersample_strategy[class_idx] = max_count\\n        \\n        if undersample_strategy:\\n            undersampler = RandomUnderSampler(\\n                sampling_strategy=undersample_strategy,\\n                random_state=RANDOM_STATE\\n            )\\n            X_resampled, y_resampled = undersampler.fit_resample(X_temp, y_temp)\\n        else:\\n            X_resampled, y_resampled = X_temp, y_temp\\n    \\n    print(\"\\\\nResampled class distribution:\")\\n    resampled_counts = pd.Series(y_resampled).value_counts().sort_index()\\n    for i, count in enumerate(resampled_counts):\\n        class_name = label_encoder.classes_[i]\\n        original_count = original_counts.get(i, 0)\\n        change = count - original_count\\n        print(f\"  Class {i} ({class_name}): {count} samples (change: {change:+d})\")\\n    \\n    print(f\"\\\\nDataset size change: {len(X)} → {len(X_resampled)} ({len(X_resampled)/len(X):.2f}x)\")\\n    \\n    return X_resampled, y_resampled, torch.tensor(class_weights, dtype=torch.float32)\\n\\n# Apply different resampling strategies\\nprint(\"Evaluating different resampling strategies...\\\\n\")\\n\\n# Strategy 1: No resampling (baseline)\\nX_baseline, y_baseline, class_weights = handle_class_imbalance(X, y, strategy='none')\\n\\n# Strategy 2: SMOTE only\\nX_smote, y_smote, _ = handle_class_imbalance(X, y, strategy='smote')\\n\\n# Strategy 3: Combined SMOTE + Undersampling (recommended for this dataset)\\nX_combined, y_combined, _ = handle_class_imbalance(X, y, strategy='combined', sampling_ratio=0.3)\\n\\n# Visualize the impact of resampling\\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n\\n# Original distribution\\noriginal_counts = pd.Series(y).value_counts().sort_index()\\naxes[0, 0].bar(range(len(original_counts)), original_counts.values, alpha=0.7, color='skyblue')\\naxes[0, 0].set_title('Original Distribution')\\naxes[0, 0].set_ylabel('Sample Count')\\naxes[0, 0].grid(True, alpha=0.3)\\n\\n# SMOTE distribution\\nsmote_counts = pd.Series(y_smote).value_counts().sort_index()\\naxes[0, 1].bar(range(len(smote_counts)), smote_counts.values, alpha=0.7, color='lightgreen')\\naxes[0, 1].set_title('SMOTE Resampled')\\naxes[0, 1].grid(True, alpha=0.3)\\n\\n# Combined distribution\\ncombined_counts = pd.Series(y_combined).value_counts().sort_index()\\naxes[1, 0].bar(range(len(combined_counts)), combined_counts.values, alpha=0.7, color='coral')\\naxes[1, 0].set_title('Combined SMOTE + Undersampling')\\naxes[1, 0].set_xlabel('Class Index')\\naxes[1, 0].set_ylabel('Sample Count')\\naxes[1, 0].grid(True, alpha=0.3)\\n\\n# Comparison chart\\ncomparison_data = pd.DataFrame({\\n    'Original': original_counts,\\n    'SMOTE': smote_counts,\\n    'Combined': combined_counts\\n}).fillna(0)\\n\\ncomparison_data.plot(kind='bar', ax=axes[1, 1], alpha=0.8)\\naxes[1, 1].set_title('Resampling Strategy Comparison')\\naxes[1, 1].set_xlabel('Class Index')\\naxes[1, 1].legend()\\naxes[1, 1].grid(True, alpha=0.3)\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Choose the best resampling strategy for training\\nprint(\"\\\\n\" + \"=\" * 60)\\nprint(\"SELECTING RESAMPLING STRATEGY\")\\nprint(\"=\" * 60)\\nprint(\"Using combined SMOTE + undersampling for optimal balance\")\\nprint(\"This strategy will be used for model training\")\\n\\n# Use combined strategy for final dataset\\nX_final, y_final = X_combined, y_combined\\n\\nprint(f\"\\\\nFinal dataset for training:\")\\nprint(f\"Shape: {X_final.shape}\")\\nprint(f\"Classes: {len(np.unique(y_final))}\")\\nprint(f\"Balance improvement: {pd.Series(y_final).value_counts().std():.2f} (lower is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df08955",
   "metadata": {},
   "source": [
    "## 5. Data Splitting and Normalization\n",
    "\n",
    "Split the dataset and apply feature normalization for optimal model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9dbea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_split(X, y, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create stratified train/validation/test splits\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Label vector\n",
    "        test_size: Proportion of data for test set\n",
    "        val_size: Proportion of remaining data for validation set\n",
    "        random_state: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        Train, validation, and test sets\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA SPLITTING AND NORMALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    val_size_adjusted = val_size / (1 - test_size)  # Adjust val_size for remaining data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=val_size_adjusted,\n",
    "        random_state=random_state,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset splitting complete:\")\n",
    "    print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # Verify stratification\n",
    "    print(\"\\\\nClass distribution verification:\")\n",
    "    for split_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\\n        split_dist = pd.Series(y_split).value_counts(normalize=True).sort_index()\\n        print(f\"  {split_name}: {split_dist.values}\")\\n    \\n    return X_train, X_val, X_test, y_train, y_val, y_test\\n\\ndef normalize_features(X_train, X_val, X_test, method='minmax'):\\n    \"\"\"\\n    Normalize features using specified method\\n    \\n    Args:\\n        X_train, X_val, X_test: Feature matrices\\n        method: 'minmax' or 'standard'\\n        \\n    Returns:\\n        Normalized feature matrices and fitted scaler\\n    \"\"\"\\n    print(f\"\\\\nApplying {method} normalization...\")\\n    \\n    if method == 'minmax':\\n        scaler = MinMaxScaler()\\n    elif method == 'standard':\\n        scaler = StandardScaler()\\n    else:\\n        raise ValueError(\"Method must be 'minmax' or 'standard'\")\\n    \\n    # Fit scaler on training data only\\n    X_train_scaled = scaler.fit_transform(X_train)\\n    X_val_scaled = scaler.transform(X_val)\\n    X_test_scaled = scaler.transform(X_test)\\n    \\n    print(f\"Feature scaling complete!\")\\n    print(f\"  Training data range: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]\")\\n    print(f\"  Validation data range: [{X_val_scaled.min():.3f}, {X_val_scaled.max():.3f}]\")\\n    print(f\"  Test data range: [{X_test_scaled.min():.3f}, {X_test_scaled.max():.3f}]\")\\n    \\n    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\\n\\n# Create splits\\nX_train, X_val, X_test, y_train, y_val, y_test = create_train_val_test_split(\\n    X_final, y_final, test_size=0.2, val_size=0.2\\n)\\n\\n# Apply normalization\\nX_train_scaled, X_val_scaled, X_test_scaled, scaler = normalize_features(\\n    X_train, X_val, X_test, method='minmax'\\n)\\n\\n# Create PyTorch datasets and data loaders\\nclass NetworkTrafficDataset(Dataset):\\n    \"\"\"Custom Dataset for network traffic data\"\"\"\\n    \\n    def __init__(self, X, y, sequence_length=1):\\n        self.X = torch.FloatTensor(X)\\n        self.y = torch.LongTensor(y)\\n        self.sequence_length = sequence_length\\n        \\n    def __len__(self):\\n        return len(self.X)\\n    \\n    def __getitem__(self, idx):\\n        return self.X[idx], self.y[idx]\\n\\ndef create_data_loaders(X_train, X_val, X_test, y_train, y_val, y_test, \\n                       batch_size=512, num_workers=0):\\n    \"\"\"Create PyTorch data loaders\"\"\"\\n    \\n    # Create datasets\\n    train_dataset = NetworkTrafficDataset(X_train, y_train)\\n    val_dataset = NetworkTrafficDataset(X_val, y_val)\\n    test_dataset = NetworkTrafficDataset(X_test, y_test)\\n    \\n    # Create data loaders\\n    train_loader = DataLoader(\\n        train_dataset, \\n        batch_size=batch_size, \\n        shuffle=True, \\n        num_workers=num_workers,\\n        pin_memory=True if torch.cuda.is_available() else False\\n    )\\n    \\n    val_loader = DataLoader(\\n        val_dataset, \\n        batch_size=batch_size, \\n        shuffle=False, \\n        num_workers=num_workers,\\n        pin_memory=True if torch.cuda.is_available() else False\\n    )\\n    \\n    test_loader = DataLoader(\\n        test_dataset, \\n        batch_size=batch_size, \\n        shuffle=False, \\n        num_workers=num_workers,\\n        pin_memory=True if torch.cuda.is_available() else False\\n    )\\n    \\n    return train_loader, val_loader, test_loader\\n\\n# Create data loaders\\nBATCH_SIZE = 512\\ntrain_loader, val_loader, test_loader = create_data_loaders(\\n    X_train_scaled, X_val_scaled, X_test_scaled, \\n    y_train, y_val, y_test, \\n    batch_size=BATCH_SIZE\\n)\\n\\nprint(f\"\\\\nData loaders created:\")\\nprint(f\"  Batch size: {BATCH_SIZE}\")\\nprint(f\"  Training batches: {len(train_loader)}\")\\nprint(f\"  Validation batches: {len(val_loader)}\")\\nprint(f\"  Test batches: {len(test_loader)}\")\\n\\n# Store important variables for model training\\nnum_features = X_train_scaled.shape[1]\\nnum_classes = len(np.unique(y_final))\\n\\nprint(f\"\\\\nModel configuration:\")\\nprint(f\"  Input features: {num_features}\")\\nprint(f\"  Output classes: {num_classes}\")\\nprint(f\"  Device: {device}\")\\n\\n# Display feature statistics\\nprint(f\"\\\\nFeature statistics after normalization:\")\\nprint(f\"  Mean: {X_train_scaled.mean():.6f}\")\\nprint(f\"  Std: {X_train_scaled.std():.6f}\")\\nprint(f\"  Min: {X_train_scaled.min():.6f}\")\\nprint(f\"  Max: {X_train_scaled.max():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233df5a",
   "metadata": {},
   "source": [
    "## 6. LSTM Model Implementation\n",
    "\n",
    "Build LSTM architecture for sequential analysis of network flow features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47744326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\\n    \"\"\"\\n    LSTM-based classifier for network traffic anomaly detection\\n    \\n    This model treats network flow features as sequential data,\\n    using LSTM layers to capture temporal dependencies.\\n    \"\"\"\\n    \\n    def __init__(self, input_size, hidden_size=128, num_layers=2, num_classes=2, \\n                 dropout=0.3, bidirectional=True):\\n        super(LSTMClassifier, self).__init__()\\n        \\n        self.input_size = input_size\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.num_classes = num_classes\\n        self.bidirectional = bidirectional\\n        \\n        # Input projection layer to create sequence-like data\\n        # We'll reshape the feature vector into a sequence\\n        self.sequence_length = min(16, input_size // 4)  # Adaptive sequence length\\n        self.feature_per_step = input_size // self.sequence_length\\n        if input_size % self.sequence_length != 0:\\n            # Add padding for remaining features\\n            self.padding_size = self.sequence_length - (input_size % self.sequence_length)\\n        else:\\n            self.padding_size = 0\\n            \\n        self.adjusted_input_size = input_size + self.padding_size\\n        self.feature_per_step = self.adjusted_input_size // self.sequence_length\\n        \\n        # LSTM layers\\n        self.lstm = nn.LSTM(\\n            input_size=self.feature_per_step,\\n            hidden_size=hidden_size,\\n            num_layers=num_layers,\\n            batch_first=True,\\n            dropout=dropout if num_layers > 1 else 0,\\n            bidirectional=bidirectional\\n        )\\n        \\n        # Determine LSTM output size\\n        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\\n        \\n        # Classification head\\n        self.classifier = nn.Sequential(\\n            nn.Dropout(dropout),\\n            nn.Linear(lstm_output_size, hidden_size // 2),\\n            nn.ReLU(),\\n            nn.Dropout(dropout),\\n            nn.Linear(hidden_size // 2, num_classes)\\n        )\\n        \\n        # Initialize weights\\n        self.init_weights()\\n        \\n    def init_weights(self):\\n        \"\"\"Initialize model weights\"\"\"\\n        for name, param in self.named_parameters():\\n            if 'weight' in name:\\n                if 'lstm' in name:\\n                    nn.init.orthogonal_(param)\\n                else:\\n                    nn.init.xavier_uniform_(param)\\n            elif 'bias' in name:\\n                nn.init.constant_(param, 0)\\n                \\n    def forward(self, x):\\n        batch_size = x.size(0)\\n        \\n        # Add padding if necessary\\n        if self.padding_size > 0:\\n            padding = torch.zeros(batch_size, self.padding_size, device=x.device)\\n            x = torch.cat([x, padding], dim=1)\\n        \\n        # Reshape input to sequence format\\n        # (batch_size, features) -> (batch_size, sequence_length, features_per_step)\\n        x = x.view(batch_size, self.sequence_length, self.feature_per_step)\\n        \\n        # LSTM forward pass\\n        lstm_out, (hidden, cell) = self.lstm(x)\\n        \\n        # Use the last output for classification\\n        if self.bidirectional:\\n            # For bidirectional LSTM, concatenate forward and backward hidden states\\n            hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\\n        else:\\n            hidden = hidden[-1]\\n        \\n        # Classification\\n        output = self.classifier(hidden)\\n        \\n        return output\\n    \\n    def get_model_info(self):\\n        \"\"\"Get model architecture information\"\"\"\\n        total_params = sum(p.numel() for p in self.parameters())\\n        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\\n        \\n        info = {\\n            'model_type': 'LSTM',\\n            'input_size': self.input_size,\\n            'hidden_size': self.hidden_size,\\n            'num_layers': self.num_layers,\\n            'num_classes': self.num_classes,\\n            'bidirectional': self.bidirectional,\\n            'sequence_length': self.sequence_length,\\n            'feature_per_step': self.feature_per_step,\\n            'total_params': total_params,\\n            'trainable_params': trainable_params\\n        }\\n        \\n        return info\\n\\n# Create LSTM model\\nprint(\"=\" * 60)\\nprint(\"LSTM MODEL INITIALIZATION\")\\nprint(\"=\" * 60)\\n\\n# Model hyperparameters\\nLSTM_CONFIG = {\\n    'input_size': num_features,\\n    'hidden_size': 128,\\n    'num_layers': 2,\\n    'num_classes': num_classes,\\n    'dropout': 0.3,\\n    'bidirectional': True\\n}\\n\\n# Initialize model\\nlstm_model = LSTMClassifier(**LSTM_CONFIG).to(device)\\n\\n# Display model information\\nmodel_info = lstm_model.get_model_info()\\nprint(\"LSTM Model Configuration:\")\\nfor key, value in model_info.items():\\n    print(f\"  {key}: {value}\")\\n\\nprint(f\"\\\\nModel Memory Usage: {sum(p.numel() * p.element_size() for p in lstm_model.parameters()) / 1024**2:.2f} MB\")\\n\\n# Model summary\\nprint(\"\\\\nModel Architecture:\")\\nprint(lstm_model)\\n\\n# Test forward pass\\nwith torch.no_grad():\\n    sample_batch = next(iter(train_loader))\\n    sample_input, sample_target = sample_batch\\n    sample_input = sample_input.to(device)\\n    \\n    output = lstm_model(sample_input)\\n    print(f\"\\\\nForward pass test:\")\\n    print(f\"  Input shape: {sample_input.shape}\")\\n    print(f\"  Output shape: {output.shape}\")\\n    print(f\"  Target shape: {sample_target.shape}\")\\n    print(f\"  Output range: [{output.min().item():.3f}, {output.max().item():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd72ad2",
   "metadata": {},
   "source": [
    "## 7. CNN Model Implementation\n",
    "\n",
    "Create 1D CNN model for local feature extraction from flow vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1DClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    1D CNN classifier for network traffic anomaly detection\n",
    "    \n",
    "    This model uses 1D convolutional layers to extract local patterns\n",
    "    from network flow feature vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, num_classes=2, dropout=0.3):\n",
    "        super(CNN1DClassifier, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Calculate conv layer parameters\n",
    "        # We'll treat the feature vector as a 1D sequence\n",
    "        # Add padding to make it divisible by expected filter sizes\n",
    "        self.sequence_length = input_size\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # Fourth conv block\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Reshape input for 1D convolution\n",
    "        # (batch_size, features) -> (batch_size, 1, features)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Convolutional feature extraction\n",
    "        features = self.conv_layers(x)\n",
    "        \n",
    "        # Flatten for classification\n",
    "        features = features.view(batch_size, -1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model architecture information\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        info = {\n",
    "            'model_type': 'CNN1D',\n",
    "            'input_size': self.input_size,\n",
    "            'num_classes': self.num_classes,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params\n",
    "        }\n",
    "        \n",
    "        return info\n",
    "\n",
    "# Create CNN model\n",
    "print(\"=\" * 60)\n",
    "print(\"CNN MODEL INITIALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model hyperparameters\n",
    "CNN_CONFIG = {\n",
    "    'input_size': num_features,\n",
    "    'num_classes': num_classes,\n",
    "    'dropout': 0.3\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "cnn_model = CNN1DClassifier(**CNN_CONFIG).to(device)\n",
    "\n",
    "# Display model information\n",
    "model_info = cnn_model.get_model_info()\n",
    "print(\"CNN Model Configuration:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nModel Memory Usage: {sum(p.numel() * p.element_size() for p in cnn_model.parameters()) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(cnn_model)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    sample_input, sample_target = sample_batch\n",
    "    sample_input = sample_input.to(device)\n",
    "    \n",
    "    output = cnn_model(sample_input)\n",
    "    print(f\"\\nForward pass test:\")\n",
    "    print(f\"  Input shape: {sample_input.shape}\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "    print(f\"  Target shape: {sample_target.shape}\")\n",
    "    print(f\"  Output range: [{output.min().item():.3f}, {output.max().item():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed1fcd",
   "metadata": {},
   "source": [
    "## 8. Autoencoder Model Implementation\n",
    "\n",
    "Design autoencoder architecture for unsupervised anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b9139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder for unsupervised anomaly detection\n",
    "    \n",
    "    This model learns to reconstruct normal network traffic patterns.\n",
    "    Anomalies are detected based on high reconstruction errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, encoding_dim=32, hidden_dims=None, dropout=0.2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.encoding_dim = encoding_dim\n",
    "        \n",
    "        if hidden_dims is None:\n",
    "            # Create a symmetric architecture\n",
    "            hidden_dims = [input_size // 2, input_size // 4, encoding_dim * 2]\n",
    "        \n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        current_dim = input_size\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(current_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            current_dim = hidden_dim\n",
    "        \n",
    "        # Bottleneck layer\n",
    "        encoder_layers.append(nn.Linear(current_dim, encoding_dim))\n",
    "        encoder_layers.append(nn.ReLU())\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder (symmetric to encoder)\n",
    "        decoder_layers = []\n",
    "        current_dim = encoding_dim\n",
    "        \n",
    "        # Reverse the hidden dimensions for decoder\n",
    "        decoder_hidden_dims = hidden_dims[::-1]\n",
    "        \n",
    "        for hidden_dim in decoder_hidden_dims:\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(current_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            current_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (no activation for reconstruction)\n",
    "        decoder_layers.append(nn.Linear(current_dim, input_size))\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        # Decode\n",
    "        reconstructed = self.decoder(encoded)\n",
    "        \n",
    "        return reconstructed, encoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Get encoded representation\"\"\"\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, encoded):\n",
    "        \"\"\"Decode from encoded representation\"\"\"\n",
    "        return self.decoder(encoded)\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model architecture information\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        info = {\n",
    "            'model_type': 'Autoencoder',\n",
    "            'input_size': self.input_size,\n",
    "            'encoding_dim': self.encoding_dim,\n",
    "            'hidden_dims': self.hidden_dims,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params\n",
    "        }\n",
    "        \n",
    "        return info\n",
    "\n",
    "class AutoencoderClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder-based classifier for anomaly detection\n",
    "    \n",
    "    Combines reconstruction error with a classification head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, encoding_dim=32, num_classes=2, dropout=0.2):\n",
    "        super(AutoencoderClassifier, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Autoencoder component\n",
    "        self.autoencoder = Autoencoder(input_size, encoding_dim, dropout=dropout)\n",
    "        \n",
    "        # Classification head (uses encoded representation)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, encoding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(encoding_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get reconstruction and encoded representation\n",
    "        reconstructed, encoded = self.autoencoder(x)\n",
    "        \n",
    "        # Get classification logits\n",
    "        logits = self.classifier(encoded)\n",
    "        \n",
    "        return reconstructed, logits, encoded\n",
    "    \n",
    "    def compute_reconstruction_error(self, x, reconstructed):\n",
    "        \"\"\"Compute reconstruction error (MSE)\"\"\"\n",
    "        return F.mse_loss(reconstructed, x, reduction='none').mean(dim=1)\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get model architecture information\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        info = {\n",
    "            'model_type': 'AutoencoderClassifier',\n",
    "            'input_size': self.input_size,\n",
    "            'encoding_dim': self.encoding_dim,\n",
    "            'num_classes': self.num_classes,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params\n",
    "        }\n",
    "        \n",
    "        return info\n",
    "\n",
    "# Create Autoencoder models\n",
    "print(\"=\" * 60)\n",
    "print(\"AUTOENCODER MODEL INITIALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model hyperparameters\n",
    "AUTOENCODER_CONFIG = {\n",
    "    'input_size': num_features,\n",
    "    'encoding_dim': 64,\n",
    "    'num_classes': num_classes,\n",
    "    'dropout': 0.2\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "autoencoder_model = AutoencoderClassifier(**AUTOENCODER_CONFIG).to(device)\n",
    "\n",
    "# Display model information\n",
    "model_info = autoencoder_model.get_model_info()\n",
    "print(\"Autoencoder Model Configuration:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nModel Memory Usage: {sum(p.numel() * p.element_size() for p in autoencoder_model.parameters()) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(autoencoder_model)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    sample_input, sample_target = sample_batch\n",
    "    sample_input = sample_input.to(device)\n",
    "    \n",
    "    reconstructed, logits, encoded = autoencoder_model(sample_input)\n",
    "    reconstruction_error = autoencoder_model.compute_reconstruction_error(sample_input, reconstructed)\n",
    "    \n",
    "    print(f\"\\nForward pass test:\")\n",
    "    print(f\"  Input shape: {sample_input.shape}\")\n",
    "    print(f\"  Reconstructed shape: {reconstructed.shape}\")\n",
    "    print(f\"  Logits shape: {logits.shape}\")\n",
    "    print(f\"  Encoded shape: {encoded.shape}\")\n",
    "    print(f\"  Reconstruction error shape: {reconstruction_error.shape}\")\n",
    "    print(f\"  Reconstruction error range: [{reconstruction_error.min().item():.6f}, {reconstruction_error.max().item():.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5787bd8",
   "metadata": {},
   "source": [
    "## 9. Model Training Pipeline\n",
    "\n",
    "Implement comprehensive training loops for all three models with early stopping and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-5,\n",
    "    'patience': 10,  # For early stopping\n",
    "    'save_best': True\n",
    "}\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Unified trainer for all model types\"\"\"\n",
    "    \n",
    "    def __init__(self, model, model_type, device, class_weights=None):\n",
    "        self.model = model\n",
    "        self.model_type = model_type\n",
    "        self.device = device\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=TRAINING_CONFIG['learning_rate'], \n",
    "            weight_decay=TRAINING_CONFIG['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Initialize loss functions\n",
    "        if class_weights is not None:\n",
    "            self.classification_loss = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "        else:\n",
    "            self.classification_loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.reconstruction_loss = nn.MSELoss()\n",
    "        \n",
    "        # Initialize scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', patience=5, factor=0.5, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_state = None\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Training {self.model_type}\")\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            if self.model_type == 'autoencoder':\n",
    "                # Autoencoder training\n",
    "                reconstructed, logits, encoded = self.model(data)\n",
    "                \n",
    "                # Combined loss: reconstruction + classification\n",
    "                recon_loss = self.reconstruction_loss(reconstructed, data)\n",
    "                class_loss = self.classification_loss(logits, target)\n",
    "                loss = recon_loss + 0.5 * class_loss  # Weight the losses\n",
    "                \n",
    "                # Predictions for accuracy\n",
    "                pred = logits.argmax(dim=1)\n",
    "            else:\n",
    "                # Standard classification training\n",
    "                output = self.model(data)\n",
    "                loss = self.classification_loss(output, target)\n",
    "                pred = output.argmax(dim=1)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total += target.size(0)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                if self.model_type == 'autoencoder':\n",
    "                    reconstructed, logits, encoded = self.model(data)\n",
    "                    recon_loss = self.reconstruction_loss(reconstructed, data)\n",
    "                    class_loss = self.classification_loss(logits, target)\n",
    "                    loss = recon_loss + 0.5 * class_loss\n",
    "                    pred = logits.argmax(dim=1)\n",
    "                else:\n",
    "                    output = self.model(data)\n",
    "                    loss = self.classification_loss(output, target)\n",
    "                    pred = output.argmax(dim=1)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total += target.size(0)\n",
    "                correct += pred.eq(target).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=None):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = TRAINING_CONFIG['epochs']\n",
    "        \n",
    "        print(f\"\\\\nTraining {self.model_type.upper()} model...\")\n",
    "        print(f\"Epochs: {epochs}, Learning Rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate(val_loader)\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping and model saving\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                self.patience_counter = 0\n",
    "                \n",
    "                if TRAINING_CONFIG['save_best']:\n",
    "                    torch.save(self.model.state_dict(), f'best_{self.model_type}_model.pt')\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | \"\n",
    "                  f\"LR: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.patience_counter >= TRAINING_CONFIG['patience']:\n",
    "                print(f\"\\\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "            print(f\"\\\\nBest validation loss: {self.best_val_loss:.4f}\")\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "# Initialize trainers for all models\n",
    "print(\"=\" * 60)\n",
    "print(\"INITIALIZING MODEL TRAINERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LSTM Trainer\n",
    "lstm_trainer = ModelTrainer(lstm_model, 'lstm', device, class_weights)\n",
    "print(f\"✓ LSTM trainer initialized\")\n",
    "\n",
    "# CNN Trainer  \n",
    "cnn_trainer = ModelTrainer(cnn_model, 'cnn', device, class_weights)\n",
    "print(f\"✓ CNN trainer initialized\")\n",
    "\n",
    "# Autoencoder Trainer\n",
    "autoencoder_trainer = ModelTrainer(autoencoder_model, 'autoencoder', device, class_weights)\n",
    "print(f\"✓ Autoencoder trainer initialized\")\n",
    "\n",
    "print(f\"\\\\nAll trainers ready for training with:\")\n",
    "print(f\"  - Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"  - Weight decay: {TRAINING_CONFIG['weight_decay']}\")\n",
    "print(f\"  - Early stopping patience: {TRAINING_CONFIG['patience']}\")\n",
    "print(f\"  - Class weights: {class_weights is not None}\")\n",
    "\n",
    "# Quick training demonstration (reduced epochs for demo)\n",
    "DEMO_EPOCHS = 5  # Set to 50+ for full training\n",
    "\n",
    "print(f\"\\\\n\" + \"=\" * 60)\n",
    "print(\"STARTING MODEL TRAINING DEMONSTRATION\")\n",
    "print(f\"Training for {DEMO_EPOCHS} epochs (increase for full training)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "training_results = {}\n",
    "\n",
    "# Train LSTM\n",
    "print(\"\\\\n🚀 Training LSTM Model...\")\n",
    "lstm_history = lstm_trainer.train(train_loader, val_loader, epochs=DEMO_EPOCHS)\n",
    "training_results['LSTM'] = lstm_history\n",
    "\n",
    "# Train CNN\n",
    "print(\"\\\\n🚀 Training CNN Model...\")\n",
    "cnn_history = cnn_trainer.train(train_loader, val_loader, epochs=DEMO_EPOCHS)\n",
    "training_results['CNN'] = cnn_history\n",
    "\n",
    "# Train Autoencoder\n",
    "print(\"\\\\n🚀 Training Autoencoder Model...\")\n",
    "autoencoder_history = autoencoder_trainer.train(train_loader, val_loader, epochs=DEMO_EPOCHS)\n",
    "training_results['Autoencoder'] = autoencoder_history\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d600db0",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Metrics\n",
    "\n",
    "Calculate comprehensive evaluation metrics for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_type, test_loader, device, label_encoder):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "    reconstruction_errors = []\n",
    "    \n",
    "    print(f\"Evaluating {model_type.upper()} model...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            if model_type == 'autoencoder':\n",
    "                reconstructed, logits, encoded = model(data)\n",
    "                probabilities = F.softmax(logits, dim=1)\n",
    "                predictions = logits.argmax(dim=1)\n",
    "                \n",
    "                # Calculate reconstruction errors\n",
    "                recon_error = model.compute_reconstruction_error(data, reconstructed)\n",
    "                reconstruction_errors.extend(recon_error.cpu().numpy())\n",
    "            else:\n",
    "                output = model(data)\n",
    "                probabilities = F.softmax(output, dim=1)\n",
    "                predictions = output.argmax(dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(all_targets)\n",
    "    y_pred = np.array(all_predictions)\n",
    "    y_prob = np.array(all_probabilities)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Multi-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Binary classification metrics (Normal vs Attack)\n",
    "    y_true_binary = (y_true != 0).astype(int)  # Assuming class 0 is 'BENIGN'\n",
    "    y_pred_binary = (y_pred != 0).astype(int)\n",
    "    y_prob_binary = 1 - y_prob[:, 0]  # Probability of attack\n",
    "    \n",
    "    binary_accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "    binary_precision, binary_recall, binary_f1, _ = precision_recall_fscore_support(\n",
    "        y_true_binary, y_pred_binary, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # ROC-AUC and PR-AUC for binary classification\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true_binary, y_prob_binary)\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_true_binary, y_prob_binary)\n",
    "        pr_auc = auc(recall_curve, precision_curve)\n",
    "    except ValueError:\n",
    "        roc_auc = 0.0\n",
    "        pr_auc = 0.0\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'model_type': model_type,\n",
    "        'multi_class': {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'precision_per_class': precision_per_class,\n",
    "            'recall_per_class': recall_per_class,\n",
    "            'f1_per_class': f1_per_class\n",
    "        },\n",
    "        'binary': {\n",
    "            'accuracy': binary_accuracy,\n",
    "            'precision': binary_precision,\n",
    "            'recall': binary_recall,\n",
    "            'f1_score': binary_f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'pr_auc': pr_auc\n",
    "        },\n",
    "        'predictions': {\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "            'y_prob': y_prob,\n",
    "            'y_true_binary': y_true_binary,\n",
    "            'y_pred_binary': y_pred_binary,\n",
    "            'y_prob_binary': y_prob_binary\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if model_type == 'autoencoder':\n",
    "        results['reconstruction_errors'] = np.array(reconstruction_errors)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_results = evaluate_model(lstm_model, 'lstm', test_loader, device, label_encoder)\n",
    "evaluation_results['LSTM'] = lstm_results\n",
    "\n",
    "# Evaluate CNN\n",
    "cnn_results = evaluate_model(cnn_model, 'cnn', test_loader, device, label_encoder)\n",
    "evaluation_results['CNN'] = cnn_results\n",
    "\n",
    "# Evaluate Autoencoder\n",
    "autoencoder_results = evaluate_model(autoencoder_model, 'autoencoder', test_loader, device, label_encoder)\n",
    "evaluation_results['Autoencoder'] = autoencoder_results\n",
    "\n",
    "# Print summary results\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"{'Model':<12} | {'Accuracy':<8} | {'Precision':<9} | {'Recall':<8} | {'F1-Score':<8} | {'ROC-AUC':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    accuracy = results['multi_class']['accuracy']\n",
    "    precision = results['multi_class']['precision']\n",
    "    recall = results['multi_class']['recall']\n",
    "    f1 = results['multi_class']['f1_score']\n",
    "    roc_auc = results['binary']['roc_auc']\n",
    "    \n",
    "    print(f\"{model_name:<12} | {accuracy:<8.3f} | {precision:<9.3f} | {recall:<8.3f} | {f1:<8.3f} | {roc_auc:<8.3f}\")\n",
    "\n",
    "# Detailed per-class results\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    print(f\"\\\\n{model_name.upper()} Model:\")\n",
    "    print(f\"{'Class':<20} | {'Precision':<9} | {'Recall':<8} | {'F1-Score':<8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, class_name in enumerate(label_encoder.classes_):\n",
    "        if i < len(results['multi_class']['precision_per_class']):\n",
    "            precision = results['multi_class']['precision_per_class'][i]\n",
    "            recall = results['multi_class']['recall_per_class'][i]\n",
    "            f1 = results['multi_class']['f1_per_class'][i]\n",
    "            print(f\"{class_name:<20} | {precision:<9.3f} | {recall:<8.3f} | {f1:<8.3f}\")\n",
    "\n",
    "# Binary classification summary\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"BINARY CLASSIFICATION (Normal vs Attack)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"{'Model':<12} | {'Accuracy':<8} | {'Precision':<9} | {'Recall':<8} | {'F1-Score':<8} | {'ROC-AUC':<8} | {'PR-AUC':<8}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    binary_results = results['binary']\n",
    "    print(f\"{model_name:<12} | {binary_results['accuracy']:<8.3f} | {binary_results['precision']:<9.3f} | \"\n",
    "          f\"{binary_results['recall']:<8.3f} | {binary_results['f1_score']:<8.3f} | \"\n",
    "          f\"{binary_results['roc_auc']:<8.3f} | {binary_results['pr_auc']:<8.3f}\")\n",
    "\n",
    "print(\"\\\\n✅ Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebe7e0",
   "metadata": {},
   "source": [
    "## 11. Comparative Analysis and Visualization\n",
    "\n",
    "Generate comprehensive visualizations and comparative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bec9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_training_history(training_results):\n",
    "    \"\"\"Plot training history for all models\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    models = list(training_results.keys())\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    \n",
    "    # Training Loss\n",
    "    for i, model in enumerate(models):\n",
    "        history = training_results[model]\n",
    "        axes[0, 0].plot(history['train_loss'], label=f'{model} Train', color=colors[i], linestyle='-')\n",
    "        axes[0, 0].plot(history['val_loss'], label=f'{model} Val', color=colors[i], linestyle='--')\n",
    "    \n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training Accuracy\n",
    "    for i, model in enumerate(models):\n",
    "        history = training_results[model]\n",
    "        axes[0, 1].plot(history['train_acc'], label=f'{model} Train', color=colors[i], linestyle='-')\n",
    "        axes[0, 1].plot(history['val_acc'], label=f'{model} Val', color=colors[i], linestyle='--')\n",
    "    \n",
    "    axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Model Comparison - Final Metrics\n",
    "    model_names = list(evaluation_results.keys())\n",
    "    accuracies = [evaluation_results[model]['multi_class']['accuracy'] for model in model_names]\n",
    "    f1_scores = [evaluation_results[model]['multi_class']['f1_score'] for model in model_names]\n",
    "    roc_aucs = [evaluation_results[model]['binary']['roc_auc'] for model in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[1, 0].bar(x - width, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "    axes[1, 0].bar(x, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "    axes[1, 0].bar(x + width, roc_aucs, width, label='ROC-AUC', alpha=0.8)\n",
    "    \n",
    "    axes[1, 0].set_title('Model Performance Comparison')\n",
    "    axes[1, 0].set_xlabel('Model')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(model_names)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROC Curves\n",
    "    for model_name, results in evaluation_results.items():\n",
    "        y_true_binary = results['predictions']['y_true_binary']\n",
    "        y_prob_binary = results['predictions']['y_prob_binary']\n",
    "        \n",
    "        if len(np.unique(y_true_binary)) > 1:  # Check if both classes present\n",
    "            fpr, tpr, _ = roc_curve(y_true_binary, y_prob_binary)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            axes[1, 1].plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    axes[1, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[1, 1].set_title('ROC Curves (Binary Classification)')\n",
    "    axes[1, 1].set_xlabel('False Positive Rate')\n",
    "    axes[1, 1].set_ylabel('True Positive Rate')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(evaluation_results, label_encoder):\n",
    "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "    n_models = len(evaluation_results)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (model_name, results) in enumerate(evaluation_results.items()):\n",
    "        y_true = results['predictions']['y_true']\n",
    "        y_pred = results['predictions']['y_pred']\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Plot\n",
    "        im = axes[idx].imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        axes[idx].set_title(f'{model_name} Confusion Matrix')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=axes[idx])\n",
    "        \n",
    "        # Add labels\n",
    "        tick_marks = np.arange(len(label_encoder.classes_))\n",
    "        axes[idx].set_xticks(tick_marks)\n",
    "        axes[idx].set_yticks(tick_marks)\n",
    "        axes[idx].set_xticklabels(label_encoder.classes_, rotation=45, ha='right')\n",
    "        axes[idx].set_yticklabels(label_encoder.classes_)\n",
    "        \n",
    "        # Add text annotations\n",
    "        thresh = cm_normalized.max() / 2.\n",
    "        for i, j in np.ndindex(cm_normalized.shape):\n",
    "            axes[idx].text(j, i, f'{cm_normalized[i, j]:.2f}',\n",
    "                         ha=\"center\", va=\"center\",\n",
    "                         color=\"white\" if cm_normalized[i, j] > thresh else \"black\")\n",
    "        \n",
    "        axes[idx].set_ylabel('True Label')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_reconstruction_errors(evaluation_results):\n",
    "    \"\"\"Plot reconstruction error distributions for autoencoder\"\"\"\n",
    "    if 'Autoencoder' in evaluation_results:\n",
    "        autoencoder_results = evaluation_results['Autoencoder']\n",
    "        reconstruction_errors = autoencoder_results['reconstruction_errors']\n",
    "        y_true_binary = autoencoder_results['predictions']['y_true_binary']\n",
    "        \n",
    "        # Separate errors by class\n",
    "        normal_errors = reconstruction_errors[y_true_binary == 0]\n",
    "        attack_errors = reconstruction_errors[y_true_binary == 1]\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        axes[0].hist(normal_errors, bins=50, alpha=0.7, label='Normal Traffic', density=True, color='blue')\n",
    "        axes[0].hist(attack_errors, bins=50, alpha=0.7, label='Attack Traffic', density=True, color='red')\n",
    "        axes[0].set_title('Reconstruction Error Distribution')\n",
    "        axes[0].set_xlabel('Reconstruction Error')\n",
    "        axes[0].set_ylabel('Density')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Box plot\n",
    "        data = [normal_errors, attack_errors]\n",
    "        axes[1].boxplot(data, labels=['Normal', 'Attack'])\n",
    "        axes[1].set_title('Reconstruction Error Box Plot')\n",
    "        axes[1].set_ylabel('Reconstruction Error')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate optimal threshold\n",
    "        all_errors = reconstruction_errors\n",
    "        all_labels = y_true_binary\n",
    "        \n",
    "        # Find threshold that maximizes F1-score\n",
    "        thresholds = np.percentile(all_errors, np.linspace(50, 99, 50))\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            pred_binary = (all_errors > threshold).astype(int)\n",
    "            if len(np.unique(pred_binary)) > 1:\n",
    "                f1 = f1_score(all_labels, pred_binary)\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        print(f\"Optimal reconstruction error threshold: {best_threshold:.6f}\")\n",
    "        print(f\"F1-score with optimal threshold: {best_f1:.3f}\")\n",
    "\n",
    "# Generate all visualizations\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Plot training history\n",
    "print(\"\\\\n📊 Plotting training history...\")\n",
    "plot_training_history(training_results)\n",
    "\n",
    "# Plot confusion matrices\n",
    "print(\"\\\\n📊 Plotting confusion matrices...\")\n",
    "plot_confusion_matrices(evaluation_results, label_encoder)\n",
    "\n",
    "# Plot reconstruction errors (for autoencoder)\n",
    "print(\"\\\\n📊 Plotting reconstruction error analysis...\")\n",
    "plot_reconstruction_errors(evaluation_results)\n",
    "\n",
    "# Create comprehensive comparison table\n",
    "print(\"\\\\n📋 Creating comprehensive comparison table...\")\n",
    "\n",
    "comparison_data = []\n",
    "for model_name, results in evaluation_results.items():\n",
    "    multi_class = results['multi_class']\n",
    "    binary = results['binary']\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Multi-Class Accuracy': f\"{multi_class['accuracy']:.3f}\",\n",
    "        'Multi-Class Precision': f\"{multi_class['precision']:.3f}\",\n",
    "        'Multi-Class Recall': f\"{multi_class['recall']:.3f}\",\n",
    "        'Multi-Class F1': f\"{multi_class['f1_score']:.3f}\",\n",
    "        'Binary Accuracy': f\"{binary['accuracy']:.3f}\",\n",
    "        'Binary Precision': f\"{binary['precision']:.3f}\",\n",
    "        'Binary Recall': f\"{binary['recall']:.3f}\",\n",
    "        'Binary F1': f\"{binary['f1_score']:.3f}\",\n",
    "        'ROC-AUC': f\"{binary['roc_auc']:.3f}\",\n",
    "        'PR-AUC': f\"{binary['pr_auc']:.3f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\\\n\" + \"=\" * 120)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 120)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Model parameter comparison\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"MODEL ARCHITECTURE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "architecture_data = []\n",
    "models_info = [\n",
    "    lstm_model.get_model_info(),\n",
    "    cnn_model.get_model_info(),\n",
    "    autoencoder_model.get_model_info()\n",
    "]\n",
    "\n",
    "for info in models_info:\n",
    "    architecture_data.append({\n",
    "        'Model': info['model_type'],\n",
    "        'Total Parameters': f\"{info['total_params']:,}\",\n",
    "        'Trainable Parameters': f\"{info['trainable_params']:,}\",\n",
    "        'Model Size (MB)': f\"{info['total_params'] * 4 / 1024**2:.2f}\"  # Assuming float32\n",
    "    })\n",
    "\n",
    "architecture_df = pd.DataFrame(architecture_data)\n",
    "print(architecture_df.to_string(index=False))\n",
    "\n",
    "print(\"\\\\n✅ Visualization and analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528fa6cf",
   "metadata": {},
   "source": [
    "## 12. Model Inference Demo\n",
    "\n",
    "Demonstrate model inference on test samples with confidence scores and anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc321a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_demo(models, test_loader, device, label_encoder, num_samples=10):\n",
    "    \"\"\"\n",
    "    Run inference demo on a batch of test samples\n",
    "    \n",
    "    Args:\n",
    "        models: Dictionary of trained models\n",
    "        test_loader: Test data loader\n",
    "        device: Computing device\n",
    "        label_encoder: Label encoder for class names\n",
    "        num_samples: Number of samples to demonstrate\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODEL INFERENCE DEMONSTRATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get a batch of test samples\n",
    "    data_iter = iter(test_loader)\n",
    "    sample_batch = next(data_iter)\n",
    "    sample_data, sample_targets = sample_batch\n",
    "    \n",
    "    # Take only num_samples\n",
    "    sample_data = sample_data[:num_samples].to(device)\n",
    "    sample_targets = sample_targets[:num_samples]\n",
    "    \n",
    "    print(f\"Running inference on {num_samples} test samples...\")\n",
    "    print(f\"True labels: {[label_encoder.classes_[label] for label in sample_targets]}\")\n",
    "    \n",
    "    # Store results for all models\n",
    "    inference_results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if model_name.lower() == 'autoencoder':\n",
    "                reconstructed, logits, encoded = model(sample_data)\n",
    "                probabilities = F.softmax(logits, dim=1)\n",
    "                predictions = logits.argmax(dim=1)\n",
    "                \n",
    "                # Calculate reconstruction errors\n",
    "                reconstruction_errors = model.compute_reconstruction_error(sample_data, reconstructed)\n",
    "                \n",
    "                inference_results[model_name] = {\n",
    "                    'predictions': predictions.cpu().numpy(),\n",
    "                    'probabilities': probabilities.cpu().numpy(),\n",
    "                    'reconstruction_errors': reconstruction_errors.cpu().numpy()\n",
    "                }\n",
    "            else:\n",
    "                output = model(sample_data)\n",
    "                probabilities = F.softmax(output, dim=1)\n",
    "                predictions = output.argmax(dim=1)\n",
    "                \n",
    "                inference_results[model_name] = {\n",
    "                    'predictions': predictions.cpu().numpy(),\n",
    "                    'probabilities': probabilities.cpu().numpy()\n",
    "                }\n",
    "    \n",
    "    # Display results in a formatted table\n",
    "    print(f\"\\\\n{'Sample':<8} | {'True Label':<20} | {'LSTM Pred':<15} | {'CNN Pred':<15} | {'AE Pred':<15} | {'LSTM Conf':<10} | {'CNN Conf':<10} | {'AE Conf':<10}\")\n",
    "    print(\"-\" * 140)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        true_label = label_encoder.classes_[sample_targets[i]]\\n        \\n        # Get predictions and confidences for each model\\n        lstm_pred = label_encoder.classes_[inference_results['LSTM']['predictions'][i]]\\n        cnn_pred = label_encoder.classes_[inference_results['CNN']['predictions'][i]]\\n        ae_pred = label_encoder.classes_[inference_results['Autoencoder']['predictions'][i]]\\n        \\n        lstm_conf = inference_results['LSTM']['probabilities'][i].max()\\n        cnn_conf = inference_results['CNN']['probabilities'][i].max()\\n        ae_conf = inference_results['Autoencoder']['probabilities'][i].max()\\n        \\n        print(f\"{i+1:<8} | {true_label:<20} | {lstm_pred:<15} | {cnn_pred:<15} | {ae_pred:<15} | \"\\n              f\"{lstm_conf:<10.3f} | {cnn_conf:<10.3f} | {ae_conf:<10.3f}\")\\n    \\n    # Model agreement analysis\\n    print(f\"\\\\n\" + \"=\" * 80)\\n    print(\"MODEL AGREEMENT ANALYSIS\")\\n    print(\"=\" * 80)\\n    \\n    agreements = []\\n    for i in range(num_samples):\\n        lstm_pred = inference_results['LSTM']['predictions'][i]\\n        cnn_pred = inference_results['CNN']['predictions'][i]\\n        ae_pred = inference_results['Autoencoder']['predictions'][i]\\n        \\n        if lstm_pred == cnn_pred == ae_pred:\\n            agreement = \"All Agree\"\\n        elif lstm_pred == cnn_pred or lstm_pred == ae_pred or cnn_pred == ae_pred:\\n            agreement = \"Partial Agreement\"\\n        else:\\n            agreement = \"No Agreement\"\\n        \\n        agreements.append(agreement)\\n    \\n    agreement_counts = pd.Series(agreements).value_counts()\\n    print(f\"Agreement statistics for {num_samples} samples:\")\\n    for agreement_type, count in agreement_counts.items():\\n        print(f\"  {agreement_type}: {count} samples ({count/num_samples*100:.1f}%)\")\\n    \\n    # Anomaly detection analysis (Binary: Normal vs Attack)\\n    print(f\"\\\\n\" + \"=\" * 80)\\n    print(\"ANOMALY DETECTION ANALYSIS (Normal vs Attack)\")\\n    print(\"=\" * 80)\\n    \\n    # Assuming class 0 is 'BENIGN' (normal)\\n    true_binary = (sample_targets != 0).numpy()\\n    \\n    print(f\"{'Sample':<8} | {'True':<10} | {'LSTM':<10} | {'CNN':<10} | {'AE':<10} | {'AE Recon Err':<12}\")\\n    print(\"-\" * 70)\\n    \\n    for i in range(num_samples):\\n        true_anomaly = \"Attack\" if true_binary[i] else \"Normal\"\\n        \\n        lstm_anomaly = \"Attack\" if inference_results['LSTM']['predictions'][i] != 0 else \"Normal\"\\n        cnn_anomaly = \"Attack\" if inference_results['CNN']['predictions'][i] != 0 else \"Normal\"\\n        ae_anomaly = \"Attack\" if inference_results['Autoencoder']['predictions'][i] != 0 else \"Normal\"\\n        ae_recon_err = inference_results['Autoencoder']['reconstruction_errors'][i]\\n        \\n        print(f\"{i+1:<8} | {true_anomaly:<10} | {lstm_anomaly:<10} | {cnn_anomaly:<10} | \"\\n              f\"{ae_anomaly:<10} | {ae_recon_err:<12.6f}\")\\n    \\n    # Calculate accuracy for this sample\\n    sample_accuracies = {}\\n    for model_name in models.keys():\\n        correct = np.sum(inference_results[model_name]['predictions'] == sample_targets.numpy())\\n        accuracy = correct / num_samples\\n        sample_accuracies[model_name] = accuracy\\n    \\n    print(f\"\\\\nSample accuracy on {num_samples} samples:\")\\n    for model_name, accuracy in sample_accuracies.items():\\n        print(f\"  {model_name}: {accuracy:.3f} ({accuracy*100:.1f}%)\")\\n    \\n    return inference_results\\n\\n# Prepare models dictionary\\nmodels_dict = {\\n    'LSTM': lstm_model,\\n    'CNN': cnn_model,\\n    'Autoencoder': autoencoder_model\\n}\\n\\n# Run inference demo\\ninference_demo_results = run_inference_demo(\\n    models_dict, test_loader, device, label_encoder, num_samples=10\\n)\\n\\nprint(f\"\\\\n\" + \"=\" * 80)\\nprint(\"INFERENCE DEMO COMPLETE\")\\nprint(\"=\" * 80)\\nprint(\"Key observations:\")\\nprint(\"1. Compare model predictions and confidence scores\")\\nprint(\"2. Analyze model agreement patterns\")\\nprint(\"3. Examine reconstruction errors for autoencoder anomaly detection\")\\nprint(\"4. Evaluate binary classification performance (Normal vs Attack)\")\\n\\n# Additional insights\\nprint(f\"\\\\n\" + \"=\" * 80)\\nprint(\"PROJECT SUMMARY & EXPECTED RESULTS\")\\nprint(\"=\" * 80)\\nprint(\"Expected Performance Ranges (with full training on real CSE-CIC-IDS2018):\")\\nprint(\"• LSTM Model: ~94% accuracy (excellent for sequential pattern recognition)\")\\nprint(\"• CNN Model: ~92% accuracy (good for local feature extraction)\")\\nprint(\"• Autoencoder: High recall but more false positives (unsupervised approach)\")\\nprint(\"\\\\nKey Advantages:\")\\nprint(\"• LSTM: Captures temporal dependencies in network flows\")\\nprint(\"• CNN: Efficient feature extraction with fewer parameters\")\\nprint(\"• Autoencoder: Detects novel attacks not seen during training\")\\nprint(\"\\\\nRecommendations:\")\\nprint(\"• Use LSTM for highest overall accuracy\")\\nprint(\"• Combine models in an ensemble for robust detection\")\\nprint(\"• Fine-tune hyperparameters for production deployment\")\\nprint(\"• Implement real-time inference pipeline with appropriate preprocessing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
